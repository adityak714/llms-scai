{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 1 - Gemini API\n",
    "This path will guide you on the set-up and usage of Google Gemini's API.\n",
    "\n",
    "### 0. Get and store the API key\n",
    "First of all, you need to login with your google account and get an API key [here](https://aistudio.google.com/app/apikey). It is **very important** that you do not share your API key with anyone and that you do not have it in your Repository.\n",
    "\n",
    "You can keep your API key in a secure local document and access it when needed. It is common to save the key as an environmental variable so that it can be accessed by your python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this means your API key is in plain text in your script. To avoid this, if you're using VS Code, you can add your API key to a `.env` file in your workspace root with the following line:\n",
    "\n",
    "```sh\n",
    "GEMINI_API_KEY=\"PASTE YOUR KEY HERE\"\n",
    "```\n",
    "\n",
    "Alternatively, you can use the [dot-env library](https://github.com/theskumar/python-dotenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$GEMINI_API_KEY found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758263840.616645 5965093 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "# You can check if the environment variable API_KEY has been set up properly by running this line\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "!if [ -z $GEMINI_API_KEY ]; then echo \"\\$GEMINI_API_KEY not found\"; else echo \"\\$GEMINI_API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple request\n",
    "Now, you can write a simple script to see if everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x1473ec210>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()  # here you can also pass the api_key directly using os.environ['GEMINI_API_KEY']\n",
    "config = None\n",
    "\n",
    "default_model = \"gemini-2.5-flash\"\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Ask the model to generate content about a random topic and print the response in text.\n",
    "\n",
    "Here is the [official documentation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#configure) to find the help you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from google.genai import types\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=default_model,\n",
    "    contents=\"Who are you?\",\n",
    "    config={} if config is None else config\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is an overview of the parameters that Gemini offers. Try some of them in different context and understand how they affect the final generated text.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too). Consider keeping the `max_output_tokens` to 50 so that the output is not too long; if you do, you should also set a low `thinking_budget` to avoid an empty response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp == 0 --> The Turning Torso was built by **HSB Malmö**, a cooperative housing association. They were the developer and commissioner of the project.\n",
      "\n",
      "The building was designed by the Spanish architect, structural engineer, and sculptor **Santiago Calatrava**, who also served as the structural engineer.\n",
      "temp == 0.2 --> The general contractor responsible for the construction of the Turning Torso was **NCC AB**.\n",
      "\n",
      "However, it's also important to note:\n",
      "\n",
      "*   **HSB Malmö** was the developer and client who commissioned the building.\n",
      "*   **Santiago Calatrava** was the architect who designed it.\n",
      "temp == 0.4 --> The **Turning Torso** was designed by Spanish architect **Santiago Calatrava**.\n",
      "\n",
      "It was commissioned by **HSB Malmö**, a cooperative housing association, which was also the developer and original owner of the building.\n",
      "temp == 0.6 --> The primary **developer and client** behind the Turning Torso was **HSB Malmö**, a Swedish housing cooperative.\n",
      "\n",
      "The building was **designed** by the renowned Spanish architect **Santiago Calatrava**, who also served as the structural engineer.\n",
      "\n",
      "The actual **construction** was carried out by **NCC AB**, one of the largest construction companies in the Nordic region.\n",
      "temp == 0.8 --> The main builder and developer of the Turning Torso was **HSB**, a Swedish cooperative housing association.\n",
      "\n",
      "While Santiago Calatrava was the architect who designed the building, HSB was responsible for its construction and ownership.\n",
      "temp == 1 --> The **Turning Torso** in Malmö, Sweden, was designed by Spanish architect **Santiago Calatrava**.\n",
      "\n",
      "The main contractor responsible for the physical construction was **NCC AB**, one of the largest construction companies in the Nordic region.\n",
      "\n",
      "It was commissioned and developed by **HSB Malmö**, a local housing cooperative.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "temp_vals = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "for temp in temp_vals:\n",
    "        config = types.GenerateContentConfig(\n",
    "                temperature=temp\n",
    "        )\n",
    "        response = client.models.generate_content(\n",
    "                model=default_model,\n",
    "                contents=\"Who was the builder of the Turning Torso in Malmo, Sweden?\",\n",
    "                config=config\n",
    "        )\n",
    "        print(f'temp == {temp} --> {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k == 1--> Okay, imagine you love yummy toast for breakfast, or delicious pasta for dinner, or even a cookie! All those things are made from something called **wheat**.\n",
      "\n",
      "1.  **The Farmer's Job:** A farmer is like a superhero who grows the wheat in big fields. It's a very important job!\n",
      "2.  **Sometimes it's Tricky:** But sometimes, growing wheat can be hard. Maybe it doesn't rain enough, or too much, or bugs eat some, or it just costs a lot of money to plant and harvest. If it's too hard, the farmer might say, \"Oh dear, I can't grow wheat anymore!\"\n",
      "3.  **The Government Helps:** Now, there are grown-ups called the \"government.\" They're like the big helpers for everyone in our country. They want to make sure *everyone* always has enough yummy bread and pasta to eat, and that it's not too, too expensive for mommies and daddies to buy.\n",
      "4.  **The Special Money (Subsidy!):** So, the government gives the farmers a little extra money – like a special \"helper bonus\" – to make sure they can keep growing lots and lots of wheat, even when things are tough.\n",
      "\n",
      "**Why?** So we always have enough food, the food doesn't cost too much, and our farmers can keep doing their super important job!\n",
      "top_k == 5--> Okay, imagine your tummy is rumbling, and you want some yummy toast or a sandwich. Where does that bread come from?\n",
      "\n",
      "It comes from **wheat**, and special people called **farmers** grow the wheat in big, big fields.\n",
      "\n",
      "But sometimes, growing wheat can be really hard. Maybe the sun isn't shining enough, or there's too much rain, or it costs a lot of money to buy the special machines. If it's too hard, the farmers might say, \"Oh dear, I can't grow wheat anymore!\"\n",
      "\n",
      "That's where the grown-ups who run our country, called the **government**, come in. They say, \"Hey farmers! We know it's tough. Here's some extra money to help you keep growing all that important wheat.\"\n",
      "\n",
      "**Why do they do that?**\n",
      "\n",
      "1.  **So we always have enough food!** We don't want anyone to be hungry, right? If farmers stop growing wheat, there won't be enough bread, pasta, and cookies for everyone to eat.\n",
      "2.  **To make sure bread isn't super, super expensive!** If it's easier for farmers to grow wheat, then the bread doesn't have to cost a whole lot of money, so everyone can afford to buy it.\n",
      "3.  **To make sure we can always make our own yummy food!** It's like making sure we can always make our own yummy food right here, without having to wait for it to come from very, very far away.\n",
      "\n",
      "So, the government gives money to wheat farmers like a helpful grown-up gives you a little extra help when you're trying to build a tall tower, so you don't give up and everyone can enjoy the yummy food!\n",
      "top_k == 10--> Okay, imagine the government is like the grown-ups who take care of everyone in our big town or country.\n",
      "\n",
      "And wheat farmers are like super important gardeners who grow a special plant called **wheat**.\n",
      "\n",
      "What's wheat for? It's what we use to make yummy **bread, pasta, cookies, and cereal!** So, it's super important food for everyone.\n",
      "\n",
      "Now, sometimes, growing wheat can be a bit tricky or expensive for the farmers. Maybe the weather isn't good, or their machines cost a lot.\n",
      "\n",
      "So, the government says, \"Hey farmers, we want to make sure you keep growing that important wheat!\" And they give the farmers a little bit of extra money to help them out. That extra money is called a **\"subsidy.\"**\n",
      "\n",
      "Why do they do that?\n",
      "\n",
      "1.  **So we always have food!** Imagine if farmers stopped growing wheat because it was too hard or they weren't making enough money. Then we wouldn't have any bread or cookies! So, the government gives them money to make sure they *keep* growing wheat, so we *always* have food.\n",
      "2.  **So food isn't too expensive!** If it's really hard for farmers, they might have to make bread super, super expensive. And then maybe some families couldn't buy it. The government's money helps make sure bread doesn't get *too* expensive, so everyone can afford to eat.\n",
      "3.  **To help the farmers!** It's like giving them a little boost so they can keep their farms going and keep doing their important job.\n",
      "\n",
      "So, it's like when your parents give you a little extra money to buy art supplies because they want you to keep drawing beautiful pictures, and they want to make sure you always have paper and crayons!\n",
      "top_k == 20--> Okay, imagine a farmer who grows yummy wheat, which is what we use to make your bread, your cereal, and even some of your cookies!\n",
      "\n",
      "Sometimes, it's hard for the farmer. Maybe the sun didn't shine enough, or there was too much rain, or maybe the seeds cost a lot of money. If the farmer doesn't make enough money, he might say, \"Oh no, this is too hard! I can't grow wheat anymore.\"\n",
      "\n",
      "But wait! We all need bread, right? And cereal!\n",
      "\n",
      "So, the grown-ups who run our country (we call them the government) say, \"Hey Mr. Farmer! We *really* want you to keep growing wheat so everyone has yummy food. So, we're going to give you a little extra money, like a special helper allowance!\"\n",
      "\n",
      "This extra money helps the farmer buy new seeds, fix his big tractor, and keep working hard.\n",
      "\n",
      "**Why do they do this?**\n",
      "\n",
      "1.  **So we always have enough food!** If farmers stop growing wheat, there won't be enough bread for everyone.\n",
      "2.  **So bread doesn't get super, super expensive!** If there's lots of wheat, bread stays at a good price.\n",
      "3.  **To help the farmers stay happy and keep doing their important job!**\n",
      "\n",
      "So, it's like the government is giving the farmer a little boost, just to make sure we always have yummy wheat for our food!\n",
      "top_k == 30--> Okay, imagine this!\n",
      "\n",
      "You know how you love yummy bread, cereal, and pasta? All those things come from a special plant called **wheat**.\n",
      "\n",
      "**Wheat farmers** are like the superheroes who grow all that wheat in big, big fields! It's a super important job because without them, we wouldn't have our favorite foods.\n",
      "\n",
      "Sometimes, growing wheat can be tricky. Maybe the weather is bad, or it costs a lot of money to buy seeds and big tractors. If it gets too hard, the farmers might say, \"Oh no, I can't grow wheat anymore!\"\n",
      "\n",
      "That's where the **government** comes in! The government is like the grown-ups who help run our big town or country.\n",
      "\n",
      "A **subsidy** is like giving those superhero farmers a little bit of extra help or a special \"power-up\" money.\n",
      "\n",
      "**Why do they do it?**\n",
      "\n",
      "1.  **So we always have yummy food!** If farmers get help, they can keep growing wheat, and we'll always have bread for our sandwiches and cereal for breakfast. No one wants to run out of yummy food!\n",
      "2.  **So food isn't super, super expensive!** If there's lots of wheat, then bread doesn't cost too much money. If there's not enough wheat, then bread might get very, very expensive, and that's not good for anyone.\n",
      "3.  **So farmers can keep doing their important job!** It helps them buy new seeds, fix their tractors, and take care of their families.\n",
      "\n",
      "So, the government gives the farmers a little extra money to say, \"Thank you for growing our food! Please keep up the good work so everyone has enough to eat!\" It's like helping a friend so they can keep playing their favorite game!\n",
      "top_k == 40--> Okay, imagine you really, really love bread, and cookies, and pasta! Yum!\n",
      "\n",
      "Well, all that yummy stuff comes from something called **wheat**. And special people called **farmers** grow the wheat.\n",
      "\n",
      "Sometimes, growing wheat can be hard. Maybe it doesn't rain enough, or too much. Or maybe the seeds cost a lot of money.\n",
      "\n",
      "That's where the grown-ups who help run our country – we call them the **government** – come in! They give the farmers a little bit of **special helper money**.\n",
      "\n",
      "Why do they do this?\n",
      "\n",
      "1.  **So we always have enough food!** If farmers stop growing wheat because it's too hard or they don't make enough money, then we wouldn't have enough bread or cookies for *everyone* to eat! And that would be sad.\n",
      "2.  **To keep prices fair.** If there's not enough wheat, then the bread in the store might get super, super expensive, and not everyone could afford to buy it. The helper money helps keep the cost of bread from going sky-high.\n",
      "3.  **To make sure farmers keep going.** It's like saying, \"Thank you for growing our food, here's a little help so you can keep doing your important job!\"\n",
      "\n",
      "So, the government gives farmers helper money so we always have yummy bread and food, and it doesn't cost too much. It's all about making sure everyone has enough to eat!\n",
      "top_k == 50--> Imagine you *really* love toast for breakfast, or yummy pasta for dinner, or cookies for a treat!\n",
      "\n",
      "All those yummy things are made from something called **wheat**. And who grows the wheat? Special people called **farmers**!\n",
      "\n",
      "Growing wheat is super hard work. Farmers have to plant tiny seeds, make sure they get enough sun and water, keep bugs away, and then harvest it all.\n",
      "\n",
      "Sometimes, even after all that hard work, the farmer might not make enough money to buy new seeds for next year, or fix their tractor, or even buy food for their own family.\n",
      "\n",
      "This is where the **government** (those are like the grown-ups who help run our whole country) steps in! They give the farmers a little bit of extra money, like a special \"thank you\" or a \"helping hand\" present.\n",
      "\n",
      "**Why do they do that?**\n",
      "\n",
      "1.  **So we always have food!** If farmers don't make enough money, they might get sad and stop growing wheat. Then, no more bread or pasta! The government wants to make sure we *always* have enough yummy food to eat.\n",
      "2.  **So food isn't too expensive!** If there's not enough wheat, then bread might cost *lots* and *lots* of money, and your mommy and daddy might not be able to buy it. The government helps so bread stays a fair price for everyone.\n",
      "3.  **To help the farmers keep going!** It's important to help the people who do such important work. This extra money helps them keep their farms running, buy what they need, and keep growing wheat year after year.\n",
      "\n",
      "So, it's like when you help your friend build a really big tower, so everyone can play with it and have fun! The government helps the farmers so *everyone* in the country always has yummy food.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "top_k_vals = [1,5,10,20,30,40,50]\n",
    "\n",
    "for val in top_k_vals:\n",
    "    config = types.GenerateContentConfig(\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            top_k=val\n",
    "            )\n",
    "    response = client.models.generate_content(\n",
    "                model=default_model,\n",
    "                contents=\"Explain to me like I am 5: Why would a government grant subsidy to wheat farmers?\",\n",
    "                config=config\n",
    "                )\n",
    "    print(f'top_k == {val}--> {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p == 0.1 --> Okay, imagine the grown-ups who help run our whole country are like the super-helpers for everyone! That's the **government**.\n",
      "\n",
      "Now, imagine the people who plant tiny seeds and grow big fields of wheat. That's the **wheat farmers**. Wheat is super important because it makes flour, and flour makes yummy bread, cookies, and cereal!\n",
      "\n",
      "Sometimes, it's really hard for the farmers. Maybe the weather is bad, or it costs a lot of money to buy seeds and big tractors. If it's too hard, they might stop growing wheat.\n",
      "\n",
      "So, a **subsidy** is like the government giving the farmers a little extra money, like a special helper allowance!\n",
      "\n",
      "Why do they do this?\n",
      "\n",
      "1.  **So we always have enough food!** If farmers stop growing wheat, we wouldn't have enough bread or cereal to eat. The government wants to make sure everyone has yummy food on their table.\n",
      "2.  **To make sure food isn't too expensive!** If there's not enough wheat, bread might cost a super, super lot of money. The government helps the farmers so that bread and cereal don't get too pricey for families to buy.\n",
      "3.  **To help the farmers keep doing their important job!** It helps them buy what they need and keep growing all that wheat for us.\n",
      "\n",
      "So, it's like the government is saying, \"Thank you for growing our food! Here's a little extra help so you can keep doing your amazing job, and we can all have yummy bread!\"\n",
      "top_p == 0.3 --> Okay, imagine the grown-ups who help run our whole country are like the grown-ups in your house, but for *everyone*! We'll call them the **Government**.\n",
      "\n",
      "Now, imagine there are special people called **Wheat Farmers**. They plant tiny seeds and grow big fields of wheat. What do we make from wheat? Yummy bread, cereal, cookies, and pasta! It's super important food.\n",
      "\n",
      "Sometimes, growing wheat can be really hard. Maybe the sun doesn't shine enough, or there's too much rain, or bugs eat the plants. This means the farmers might not grow as much wheat, or they might not make enough money to keep planting next year.\n",
      "\n",
      "So, the **Government** gives the **Wheat Farmers** a little extra money. Think of it like giving them a special \"helper-money\" or a \"bonus prize\" just for doing their important job. This is called a **subsidy**.\n",
      "\n",
      "Why do they do this? For three super important reasons:\n",
      "\n",
      "1.  **So we always have enough yummy food!** If farmers can't grow enough wheat, then we won't have enough bread or cereal for everyone. The helper-money makes sure they can keep planting and growing lots of wheat, so our tummies are always full.\n",
      "\n",
      "2.  **So food doesn't get too, too expensive!** If it's really hard to grow wheat, then bread might cost a lot of money. The helper-money helps keep the price of bread and cereal from going super high, so all families can afford to buy it.\n",
      "\n",
      "3.  **To help the farmers keep their jobs!** Farming is hard work! The helper-money makes sure the farmers can still buy new seeds, fix their tractors, and keep their farms going, even when things are tough. We need our farmers!\n",
      "\n",
      "So, the government gives money to wheat farmers to make sure we always have enough yummy, affordable bread and cereal, and to help the farmers keep doing their very important job!\n",
      "top_p == 0.5 --> Okay, imagine your favorite sandwich or toast for breakfast. Yummy, right?\n",
      "\n",
      "Now, who makes that bread possible? Farmers! They grow the wheat.\n",
      "\n",
      "Sometimes, it's hard for farmers to grow enough wheat or sell it for enough money to take care of their farms and their families. Maybe the weather was bad, or maybe the price of wheat went down.\n",
      "\n",
      "The **government** (that's like the grown-ups who help run our whole country) knows that **everyone needs to eat!** And wheat is super important because it makes bread, pasta, and lots of other yummy things we eat every day.\n",
      "\n",
      "So, the government gives the wheat farmers a little **extra money**. It's like giving them a special helper allowance!\n",
      "\n",
      "**Why do they do this?**\n",
      "\n",
      "1.  **So we always have enough food:** This extra money helps the farmers keep planting their seeds, buying their tractors, and working hard. It means they don't have to worry so much if the weather is tricky or if the price of wheat isn't very high.\n",
      "2.  **So food isn't too expensive:** If farmers stopped growing wheat because it was too hard, then bread would become super, super expensive because there wouldn't be much of it. The extra money helps keep the price of bread fair for everyone.\n",
      "\n",
      "So, it's like the government saying, \"Thank you for growing our food! Here's some help so you can keep doing your important job, and everyone can have enough to eat!\"\n",
      "top_p == 0.7 --> Okay, imagine the grown-ups who run our whole country – like your parents, but for *everyone*! That's the government.\n",
      "\n",
      "And you know farmers? They're the super important people who grow all the yummy food we eat, like the wheat that makes your bread, cereal, and pasta!\n",
      "\n",
      "Sometimes, growing food is really, really hard. What if there's too much rain, or not enough? Or yucky bugs eat the plants? Farmers might not make enough money to keep growing food for us.\n",
      "\n",
      "So, the government gives them a special kind of \"helping hand\" money. We call it a \"subsidy\"!\n",
      "\n",
      "**Why?**\n",
      "\n",
      "1.  **So we always have enough food!** Imagine if farmers stopped growing wheat because it was too hard or they weren't making enough money. Then we wouldn't have any bread or cereal! The government wants to make super sure we always have enough yummy food to eat.\n",
      "\n",
      "2.  **To help farmers when things are tough.** Like when you need a little help building a tall block tower, the government helps farmers so they can keep going, even if the weather is bad or prices are low.\n",
      "\n",
      "3.  **To keep food from getting too expensive.** If it's really hard to grow wheat, it might cost a lot more money to buy bread. The subsidy helps keep the price of bread fair, so everyone can afford it.\n",
      "\n",
      "So, it's like the government is saying, \"Thank you for growing our food, farmers! Here's some extra help so you can keep doing your important job, and we can all keep eating yummy bread!\"\n",
      "top_p == 0.9 --> Okay, imagine you love toast for breakfast, or yummy pasta for dinner, or a sandwich for lunch. All those things are made from something called **wheat**!\n",
      "\n",
      "Big, strong farmers grow the wheat in huge fields. But sometimes, growing wheat can be really, really hard.\n",
      "\n",
      "*   Maybe the sun doesn't shine enough, or it rains too much.\n",
      "*   Or maybe the seeds cost a lot of money, or the machines they use break down.\n",
      "\n",
      "If it's too hard, farmers might say, \"This is too much work, I'm going to stop growing wheat.\" And then... oh no! No more toast or pasta for us!\n",
      "\n",
      "So, the **government** – those are like the super-duper grown-ups who help run our whole country – they say, \"Hey farmers! We know it's tough.\"\n",
      "\n",
      "They give the farmers a little bit of **extra money**. It's like giving them a special helper allowance!\n",
      "\n",
      "**Why do they do that?**\n",
      "\n",
      "1.  **So the farmers can keep growing lots and lots of wheat.**\n",
      "2.  **So we always have enough yummy bread and pasta to eat.**\n",
      "3.  **And so the bread doesn't get super, super expensive, and everyone can still buy it.**\n",
      "\n",
      "It's like saying, \"Thank you for growing our food, here's a little help so you can keep doing it!\"\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "top_p_vals = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "for values in top_p_vals:\n",
    "    config = types.GenerateContentConfig(\n",
    "            temperature=0.4,\n",
    "            top_p=values,\n",
    "            top_k=20\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "                model=default_model,\n",
    "                contents=\"Explain to me like I am 5: Why would a government grant subsidy to wheat farmers?\",\n",
    "                config=config\n",
    "                )\n",
    "    print(f'top_p == {values} --> {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "\n",
    "#### Exercise 5\n",
    "Gemini, beside text also accepts images (and videos). Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Two men are in what appears to be an office or lab environment. On the right, a man with bilateral leg prostheses and a prosthetic left arm is seated in a wheelchair. He is wearing a blue shirt and khaki shorts. Another man, wearing a black t-shirt and dark pants, is standing or sitting at a tall table to the left. He is assisting the man in the wheelchair by adjusting or examining a part of his left arm prosthetic, specifically near the elbow. The man on the left also appears to be wearing a specialized glove or prosthetic on his left hand. A laptop is visible on the table. Large windows with blinds are in the background, providing natural light to the bright room.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"./data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Your code here\n",
    "image = Image.open(IMAGE_PATH)\n",
    "text = \"Give a description of what you see in the provided image.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=default_model,\n",
    "    contents=[text, image, \"Caption this image.\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\"\n",
    "            )\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. This becomes especially relevant if you have many documents that cannot possibly fit into the model's context window. To more easily implement a RAG pipeline we recommend the use of one of these libraries: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use *LangChain*.\n",
    "\n",
    "It can be useful to split this exercise into these steps:\n",
    "1. Read one or more documents using pdfminer\n",
    "2. Split the documents into small chunks\n",
    "3. Get and store the embeddings for each chunks\n",
    "5. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM\n",
    "\n",
    "**NOTE:** if you try to embed too many documents at once or too large documents you may run into rate limits. Possible solutions: \n",
    "* Reduce the number of chunks and/or their size\n",
    "* Look at the HF version of this lab and use a local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/miniconda3/envs/llm-2/lib/python3.11/site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/miniconda3/envs/llm-2/lib/python3.11/site-packages (from pdfminer.six) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/envs/llm-2/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/miniconda3/envs/llm-2/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20250506\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$GOOGLE_API_KEY found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758266323.115763 5965093 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "!if [ -z $GOOGLE_API_KEY ]; then echo \"\\$GOOGLE_API_KEY not found\"; else echo \"\\$GOOGLE_API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat3' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat4' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'pgfpat7' is an invalid float value\n",
      "E0000 00:00:1758266910.208728 5965093 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1758266910.209098 5965093 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os  # langchain expects gemini's api key to be in the environment variable GOOGLE_API_KEY, use os to set it\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # get embeddings from Gemini\n",
    "from langchain_community.vectorstores import FAISS  # \"db\" to store and retrieve embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # split long documents\n",
    "from pdfminer.high_level import extract_text  # extract text from pdfs\n",
    "from uuid import uuid4\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Suppose a user query\n",
    "USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "# Your code here\n",
    "# 1. Read one or more documents using pdfminer\n",
    "text = extract_text(DOC_PATH)\n",
    "# text2 = extract_text('data/RP.pdf')\n",
    "\n",
    "# 2. Split the documents into small chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=32,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "chunks_collection = text_splitter.split_text(text)\n",
    "\n",
    "# A smaller version of the collection of text chunks (Gemini has API Rate limits)\n",
    "small_cc = chunks_collection[:40]\n",
    "\n",
    "# 3. Get and store the embeddings for each chunks\n",
    "embedder = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "embeddings = embedder.embed_documents(small_cc)\n",
    "\n",
    "# 4. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedder,\n",
    "    index=faiss.IndexFlatL2(3072),\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/langchain_google_genai/embeddings.py:285\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[0;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[1;32m    286\u001b[0m         BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1438\u001b[0m, in \u001b[0;36mGenerativeServiceClient.batch_embed_contents\u001b[0;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1438\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[1;32m   1439\u001b[0m     request,\n\u001b[1;32m   1440\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m   1441\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1442\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m   1443\u001b[0m )\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    295\u001b[0m     target,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    297\u001b[0m     sleep_generator,\n\u001b[1;32m    298\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    299\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    300\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[1;32m    157\u001b[0m         exc,\n\u001b[1;32m    158\u001b[0m         deadline,\n\u001b[1;32m    159\u001b[0m         sleep_iter,\n\u001b[1;32m    160\u001b[0m         error_list,\n\u001b[1;32m    161\u001b[0m         predicate,\n\u001b[1;32m    162\u001b[0m         on_error,\n\u001b[1;32m    163\u001b[0m         exception_factory,\n\u001b[1;32m    164\u001b[0m         timeout,\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    210\u001b[0m         error_list,\n\u001b[1;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    212\u001b[0m         original_timeout,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m chunks_in_docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mchunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m small_cc] \u001b[38;5;66;03m# chunks in Documents form\u001b[39;00m\n\u001b[1;32m      4\u001b[0m uuids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks_in_docs))] \u001b[38;5;66;03m# make a randomly generated uuid, the number of times as length of docs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mchunks_in_docs, ids\u001b[38;5;241m=\u001b[39muuids)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:279\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    278\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:340\u001b[0m, in \u001b[0;36mFAISS.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m--> 340\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_documents(texts)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add(texts, embeddings, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:248\u001b[0m, in \u001b[0;36mFAISS._embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm-2/lib/python3.11/site-packages/langchain_google_genai/embeddings.py:290\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[0;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    289\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39membeddings])\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "# Convert the list of string chunks into Document() objects, and create a list of that\n",
    "chunks_in_docs = [Document(page_content=chunk) for chunk in small_cc] # chunks in Documents form\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(chunks_in_docs))] # make a randomly generated uuid, the number of times as length of docs\n",
    "vector_store.add_documents(documents=chunks_in_docs, ids=uuids) # add them to the vector store to later be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758266935.794957 5965093 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='CoT stands for **Chain-of-Thought**.\\n\\nIt is a prompting technique used with large language models (LLMs) to improve their ability to solve complex reasoning tasks. This technique involves prompting the LLM to generate a series of intermediate reasoning steps, or a \"chain of thought,\" before arriving at the final answer.\\n\\nThis approach mimics human-like problem-solving, where complex problems are broken down into smaller, more manageable steps. By explicitly generating these steps, CoT prompting helps LLMs to perform better on tasks that require multi-step reasoning, arithmetic, or symbolic manipulation. It has been shown to be particularly effective for tasks that are challenging for LLMs when only given a direct prompt.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--1cbe141d-95b0-46b7-a61a-66c040017e93-0', usage_metadata={'input_tokens': 58, 'output_tokens': 648, 'total_tokens': 706, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 505}})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"What is CoT?\" # What is CoT?\n",
    "USER_QUERY = new_query if USER_QUERY != new_query else \"What is CoT?\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.4)\n",
    "\n",
    "docs = vector_store.similarity_search(\n",
    "    USER_QUERY,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that retrieves important information depending on the query asked, to solve a question asked by the user. You have information to help you to answer a question. You must not hallucinate. \n",
    "\n",
    "The information you know is: \n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "similar_chunks = \"\".join(doc.page_content for doc in docs)\n",
    "sys_prompt = prompt_template.format(context=similar_chunks)\n",
    "\n",
    "question = llm.invoke([\n",
    "    SystemMessage(content=sys_prompt),\n",
    "    HumanMessage(content=USER_QUERY)\n",
    "])\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore on your own\n",
    "Gemini offers a bigger range of capabilities than those provided here, begin able to automatically handle multi-turn chats is one of them. Explore them on your own!\n",
    "\n",
    "#### Exercise 7\n",
    "Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a user interface\n",
    "\n",
    "#### Exercise 8\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface).\n",
    "\n",
    "Use a ChatInterface to create a chatbot UI that let's you discuss with Gemini, then add multimodal capabilities for both Gradio and Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Edit the parameters below\n",
    "chats = {}  # store the chat history for each user (suppose multiple users)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
