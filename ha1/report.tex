\documentclass[a4paper,10pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings} %Alternative to minted
%\usepackage{minted}
\usepackage{graphicx}
\usepackage{tabto}
\newenvironment{tabs}[1]
 {\flushleft\TabPositions{#1}}
 {\endflushleft}


\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{myred}{rgb}{0.8,0.1,0.1}
\definecolor{myorange}{rgb}{0.8,0.4,0.1}
\definecolor{mypurple}{rgb}{0.6,0.4,0.8}
\definecolor{mylightgray}{rgb}{0.95,0.95,0.95}

\lstset{
  backgroundcolor=\color{mylightgray},
  basicstyle=\ttfamily\small,
  keywordstyle=\color{mypurple}\bfseries,
  commentstyle=\color{mygray}\itshape,
  stringstyle=\color{myred},
  numbers=none,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  frame=none,
  tabsize=2,
  captionpos=b,
  breaklines=true,
  breakatwhitespace=false,
  escapeinside={\%*}{*)},
  morekeywords={tsdiag,qqnorm,pacf,acf,arima,predict,log,seq,plot,hist},
  alsoletter={.},
}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\title{\textbf{1RT730} LLMs and Societal Consequences of Artificial Intelligence - Report for Hand-in Assignment 1}
\author{Aditya Khadkikar | adkh8153@student.uu.se}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
% Very short summary: Did you use Gemini or HuggingFace? What model did you use? How did you integrate multimodality? What prompt did you use? Etc...

% 1. Assess if the model can correctly classify mushrooms? (The value is in the quality of your assessment, not the result itself.) Note: the model cannot classify the parasol mushroom as all information is not visible in the image (the stipe is missing). What happens in this case? 
This report provides the implementation process of an LLM interface in Python, consisting of the UI made by Gradio, and a chatbot-style interface sending requests to the Gemini API, and printing the responses to the screen. The model is part of an assistant bot that answers questions about mushrooms, and can accept a user questions, as well as optionally images. The model's classification and prediction (asking more than once for checking the same image) capability was investigated, as well as how well it can stay on-topic for the domain of mushrooms. Different images are given to the mushroom LLM to provide info about. Some bonus personal images of mushrooms, labelled \texttt{my-mushroom*.png} were also sent, where image 1 is a mushroom found near the forest area of Campus Ångström in Uppsala, and image 2 is a mushroom found in a forest in East Stockholm. A page from a textbook in Swedish about mushrooms was also given to the model, to see how well it is able to transcribe it, in both the original language (Swedish), and later in English. 

For this assignment, Google's Gemini Large Language Model (LLM) was used, specifically, the \\ \texttt{gemini-2.0-flash} model. 

LLM models also have safety features in them, in order to prevent the wrong kinds of questions being sent, or wrong kinds of answers returned that could be politically manipulative, harmful, dangerous, ethically incorrect or inappropriate. A detection feature of when safety criteria are violated has been implemented, and an alternate response is given when the question cannot be answered by the LLM (an error response). Lastly, a discussion is also made about what can be the implications or effects of lowering safety thresholds in LLM configurations, and providing answers in potentially risky contexts.

\section{Classification Accuracy}
Assess if the model can correctly classify mushrooms? Note: the model cannot classify the parasol mushroom as the stipe is missing. What happens in this case?

Rounds of queries done for each image were 3. When the request is sent to the Gemini API, it now received an array, with the elements being 1) the original user question string, and 2) the image in PIL.Image format. From the images shown ahead, it could be seen that eventually predicted mushroom 2) correctly after an initial conversation was ongoing, about the edibility of mushroom 4). 

1) \includegraphics[width=0.22\textwidth]{data/mushroom_copper_spike.jpg}
2) \includegraphics[width=0.22\textwidth]{data/mushroom_deadly_webcap.jpg}
3) \includegraphics[width=0.22\textwidth]{data/mushroom_false_chanterelle.jpg}
4) \includegraphics[width=0.22\textwidth]{data/mushroom_parasol_topview.jpg} \\

Some additional images I tried testing it on were the following:

\begin{center}
5) \includegraphics[width=0.22\textwidth]{data/my-mushroom.png}
6) \includegraphics[width=0.22\textwidth]{data/my-mushroom-2.png} \\
\end{center}

The first image from the left is a mushroom I found near the forest area of Campus Ångström in Uppsala, and the second image is a mushroom found in a forest in East Stockholm.

For viewing the responses given by the LLM, please see the following sub-appendices in the report for the screenshots: Mushroom 1): \ref{img:1}, Mushroom 2): \ref{img:2}, Mushroom 3): \ref{img:3}, Mushroom 4): \ref{img:4}. 

For the additional mushrooms I tried: Mushroom 5): \ref{img:5}, Mushroom 6): \ref{img:6}.

Mushroom 1) was classified as Hygrophorus multiple times, 2) was sometimes misclassified as Hygrophorus as well (major risk: classified as edible), then Webcap and finally, as the Tawny grisette (Amanita), both non-edible, agreeing with the transcript. Mushroom 3) was the \textbf{most mis-classified}, with it giving the wrong common name, and genus (never provided Chanterelle, or False Chanterelle as per transcript), but simply changing the color to match the color in the image, which was yellow. This was a consistent pattern noticed, that it would simply change the color, and be accurate most of the times in just detecting elements of the mushroom, with the parts detectable being right. However, it struggled on classifying the genus and type, which is the more important task.

For mushroom 4) (Parasol), the stipe, or the stem, was missing, hence it classified the common name as "Parasol Mushroom" instead of "False Parasol". The ground truth transcript mentions "The mushroom is white with brown scales on the cap", and the detected color was "brown" by the LLM. For understanding the reason behind the false classification, I later tried to provide a new image of False Parasol, with the stipe being visible (taken from \\ \url{https://picturemushroom.com/wiki/Chlorophyllum_molybdites.html} and \\ \url{https://www.mushroom-appreciation.com/toxic-false-parasol.html}). 

It still did not recognize the correct type, and gave "Parasol Mushroom" as the common name, despite the stipe being clearly visible. Hence, if the classification specifically in this case were to be improved, where it should know the difference between a \textit{false} and \textit{true} parasol, it should be trained on different mushroom textbook pages, which mention the distinction between the two types of fungi, and characteristic differences. According to the \texttt{transcript.txt}, it does mention that some variants can cause stomach upset, and in comparison, the LLM at least warns to the user to cook it \textbf{all the way through to get rid of all potential toxins}.

\begin{center}
\includegraphics[width=0.25\textwidth]{images/parasol-new1.png}
\includegraphics[width=0.25\textwidth]{images/parasol-new2.png}
\end{center}

% Next, I compared the responses with the \texttt{groundtruth\_transcript.txt} file containing some information about the images provided. 

% mushroom_copper_spike.jpg:
%     A picture of two copper spike mushrooms growing in the forest. They are brown and
%     shiny. They are considered mediocre edible mushrooms.

% mushroom_deadly_webcap.jpg:
%     A picture of five deadly webcap mushrooms growing in the forest. They are orange
%     colors with brown snake-like patterns on the stipe. They are absolutely deadly as
%     they contain the toxin orellanine, which causes acute kidney failure. There is no
%     known antidote.

% mushroom_false_chanterelle.jpg:
%     A picture of three false chanterelle mushroom growing in the forest. They are yellow
%     and have gills. They are considered inedible as they can cause stomach upset in some
%     people. They are often mistaken for the edible chanterelle, but the chanterelles
%     have ridges instead of gills, are firmer, and have a distinctive smell.

% mushroom_parasol_topview.jpg
%     A picture of a false parasol mushroom growing on grass. The picture is taken from
%     above, so the stipe is not visible. The mushroom is white with brown scales on the
%     cap. This mushroom is edible but has variants that can cause stomach upset, like
%     the false parasol. The false parasol has a bare, white stipe, while the parasol
%     features a distinct pattern on the stipe. This image cannot be classified accurately
%     as the stipe is not visible.

% nya_svampboken_p226.jpg:
%     This is a picture of a page from the book "Nya Svampboken" by Pelle Holmberg and
%     Hans Marklund, p. 226. The page is written in Swedish and describes the deadly
%     webcap mushroom.
%     Transcript:
%     ✝ Toppig giftspindling Cortinarius rubellus (syn. C. speciosissimus)
%     Spindlingarna (spindelskivlingarna) utgör en mycket stor del av stor-
%     svamparna i en svensk skog. I tidigare svampböcker finns flear mat-
%     svampar upptagna bland dessa, men idag avråder vi från att äta spind-
%     lingar. Två av de farligaste giftsvamparna vi har i vårt land hör till detta
%     släkte, nämligen toppig giftspindling och orangebrun giftspindling
%     (s. 230).

%     Gift, symptom och behandling
%     Dessa spindlingar innehåller giftet orellanin som har en specifik njur-
%     skadande effekt.* Det otäcka med dessa giftsvampar är att tidiga för-
%     giftningssymptom saknas eller är mycket svaga. Det är först efter 3 till 7,
%     i vissa fall upp till 14, dygn, som påtagliga symptom visar sig - intensiv
%     törst, trötthet, njursmärtor och först ökande, sedan upphörande urin-
%     produktion - och då har den förgiftade ofta fått svåra njurskador. Be-
%     handlingen i detta skede innebär dialys och i svåra fall njurtransplantation.

%     Viktiga kännetecken på toppig giftspindling
%     • Hatten är matt, liksom filthårig, rödbrun till gulbrun. Hatten är ibland
%       toppig men den kan också vara välvd eller till och med tillplattad på
%       mitten.
%     • De rödbruna skivorna under hatten är tjocka, höga och tämligen glesa.
%     • Runt den bruna foten finns gula, oregelbundna band.

%     [Transcription note: The next part is written in the margin]
%     * Beträffande övriga
%     spindlingar, som i olika
%     sammanhang ofta
%     anges vara giftiga,
%     finns det ännu inga
%     vetenskapliga belägg
%     för att de skulle inne-
%     hålla något farligt gift,
%     som t.ex. orellanin. Det
%     gäller t.ex. gulbandad
%     spindling (Cortinarius
%     gentilis), eldspindling
%     (C. limonius), lokspind-
%     ling (C. callisteus) och
%     svavelspindling (C.
%     meinhardii, se äv. s. 121).

\section{Prediction Consistency}
The model is likely not consistent in its predictions. One way is to change the temperature of the model and set it to 0.0. What other causes could lead to inconsistent predictions?

With a \texttt{gr.Slider} component, there was a possibility to control the temperature of the LLM on the UI side. The default value was set to 0.7, ranging from 0.0 to 1.0 with a step size of 0.05. For this, I tried by setting the temperature to 0.0, and asking it to re-classify the most misclassified mushroom, which was 3). This time it returned:

\includegraphics[width=\textwidth]{images/mushroom3-notemp.png}

This was tried 2 more times, and it returned exactly the same JSON response. Hence, the results were reproducible, and although the identified genus and common name does not match with the False Chanterelle, it marked it as non-edible, and returned the same confidence score. Next, as another example, I tried mushroom 2), and it falsely returned Jack O'Lantern Mushroom again, with the difference only being the color now changed to "Brown" instead of the former case "Yellow". Although reproducibility is present, the model can be a bit strict, and cannot have room to think that a new species can be present, but simply be over-weighed by the difference detection in the color, which it returns directly. 

I tried resetting the history, by re-running the Gradio app, and tried asking about mushroom 2) again. It returned:

\begin{center}
\includegraphics[width=0.5\textwidth]{images/mushroom2-notemp.png} 
\includegraphics[width=0.9\textwidth]{images/mushroom2-notemp-eating.png} 
\includegraphics[width=0.9\textwidth]{images/mushroom2-notemp-causes.png}    
\end{center}

Same as in the previous case, after trying for 2 more trials, it reproduced the same JSON information about the mushroom, however the correctness of the information is also a more important criterion to be considered. Additionally, with a question asked about how to cook it, the model gives much shorter responses, as the generation of the model is restricted with the temperature. Hence, it answers more to-the-point, as shown in the image above.

As another experiment, I first asked about Mushroom 2), then asked about Mushroom 3), with temperature set to 0.0, giving the following response:

\begin{center}
    \includegraphics[width=0.9\textwidth]{images/mushroom2and3.png}
\end{center}

\section{Topic Control}
% Can you find a way to make the chatbot talk about another topic than mushrooms? Is it hard? If not, how would you make it harder?

A method to make the model talk only about the topic of mushrooms, was sufficient pre-prompting. It needs to be reminded to stick to the topic of mushroom classification, and avoid any other questions that are outside of the topic. This worked quite well, and for off-topic questions like "Tell me about the grass beside the mushroom in the image", it asked the user to ask something regarding the mushrooms. 

Extending to that, if the topic needs to be controlled, the model can be given examples of when to negate answering an off-topic question, and when to detect whether a question or statement by the user is adhering to the topic of mushrooms or not. This can look like: 

\begin{tabs}{2cm, 4cm}
\{ \\
    \tab User: What is 5 + 2? \\
    \tab LLM: I am sorry, but I am not supposed to answer mathematics-related questions. \\ \tab I am an assistant chatbot that answers your questions about mushrooms. \\ \tab Feel free to ask me anything about them! \\
\} \\
\{ \\
    \tab User: Who was the best player in the 2015 Football Match of Arsenal v.s. \\ \tab West Ham in the Premier League? \\
    \tab LLM: I am sorry, but I am not supposed to answer football-related questions. \\ \tab I am an assistant chatbot that answers your questions about mushrooms.\\
\} \\
\{ \\
    \tab User: I am having Python installation problems, why is it not being loaded correctly? \\
    \tab LLM: I am sorry, but I am not supposed to answer Python, or coding-related questions. \\ \tab I am an assistant chatbot that answers your questions about mushrooms. \\ \tab Feel free to ask me anything about them! \\
\}
\end{tabs}

This can be beneficial in bringing the LLM to only answer related to mushrooms. However, with regards to token limits, and how much info is allowed to be sent to LLMs through API requests, it may take up a lot of tokens. It may add up, and accumulate with every API request, being heavy on the token count, and hence, I implemented a single prompt, and few shot reminders like above were not implemented. It is definitely a good idea to do so if one has a sufficiently large token allowance.

\section{Transcription Quality}

The transcription done by the LLM, and the one present in transcript.txt is very consistent with each other. The translation of the same transcript was tried in English, and was also done reasonably well by the LLM. 

\textbf{The ground truth transcription is given as:}

     Toppig giftspindling Cortinarius rubellus (syn. C. speciosissimus)
    Spindlingarna (spindelskivlingarna) utgör en mycket stor del av stor-
    svamparna i en svensk skog. I tidigare svampböcker finns flear mat-
    svampar upptagna bland dessa, men idag avråder vi från att äta spind-
    lingar. Två av de farligaste giftsvamparna vi har i vårt land hör till detta
    släkte, nämligen toppig giftspindling och orangebrun giftspindling
    (s. 230).

    Gift, symptom och behandling
    Dessa spindlingar innehåller giftet orellanin som har en specifik njur-
    skadande effekt.* Det otäcka med dessa giftsvampar är att tidiga för-
    giftningssymptom saknas eller är mycket svaga. Det är först efter 3 till 7,
    i vissa fall upp till 14, dygn, som påtagliga symptom visar sig - intensiv
    törst, trötthet, njursmärtor och först ökande, sedan upphörande urin-
    produktion - och då har den förgiftade ofta fått svåra njurskador. Be-
    handlingen i detta skede innebär dialys och i svåra fall njurtransplantation.

    Viktiga kännetecken på toppig giftspindling
    • Hatten är matt, liksom filthårig, rödbrun till gulbrun. Hatten är ibland
      toppig men den kan också vara välvd eller till och med tillplattad på
      mitten.
    • De rödbruna skivorna under hatten är tjocka, höga och tämligen glesa.
    • Runt den bruna foten finns gula, oregelbundna band.

    [Transcription note: The next part is written in the margin]
    * Beträffande övriga
    spindlingar, som i olika
    sammanhang ofta
    anges vara giftiga,
    finns det ännu inga
    vetenskapliga belägg
    för att de skulle inne-
    hålla något farligt gift,
    som t.ex. orellanin. Det
    gäller t.ex. gulbandad
    spindling (Cortinarius
    gentilis), eldspindling
    (C. limonius), lokspind-
    ling (C. callisteus) och
    svavelspindling (C.
    meinhardii, se äv. s. 121).

\includegraphics[width=\textwidth]{images/transcription-swe.png}
\includegraphics[width=\textwidth]{images/transcription-eng.png}

\section{Safety Filters}
(Gemini) To transcribe the page from `Nya svampboken`, you may have had to lower the safety of the model. What are the risks and consequences of doing so?

For transcribing the image, I did not need to particularly lower the safety of the model. A detection of whether one, or multiple safety criteria were violated was done using a try-catch block in Python. From the Google Gemini safety thresholds documentation, it was mentioned that if safety criteria are violated, then a new attribute is returned in the first chunk itself (if streaming), known as \texttt{GenerateContentResponse}. Additionally, the streaming of the LLM's answer would be aborted, hence it would not be printed in the UI. Therefore, the logic of sending the user query, and streaming the response from Gemini was wrapped in a try-except. If an AttributeError was detected (the next chunk not having a \texttt{.parts} attribute in dict), then likely the model decided not to return an answer, based on its safety criteria. On the Python kernel, it is printed as the following:

\begin{center}
    \includegraphics[width=0.8\textwidth]{images/safety-filter-consoleprint.png}    
\end{center}

On the UI side of Gradio, the LLM interface returns "There was an error. Please try again." to the user. 

\includegraphics[width=\textwidth]{images/safetyresponse-llm-ui.png}

In order to test on the local side whether the safety triggers work, the safety thresholds had to be lowered to a level like BLOCK\_LOW\_AND\_ABOVE. In the Gemini documentation, there are default safety configurations that a gemini model instance has, and certain ones that the user can control, at different probability of a safety violation (BLOCK\_ONLY\_HIGH, BLOCK\_NONE, BLOCK\_MEDIUM\_AND\_ABOVE, etc), by providing them in the config of \texttt{generate\_content\_stream()}, as a parameter \texttt{safety\_settings} as a Python list [ ] of \texttt{SafetySetting} instances. The default safety configurations are still quite allowing, and still finish generating a response, but still conduct detections on the user query to see if it is a safety hazard, and responds in the lines of `I am not allowed to respond to your question, as it is unsafe' or similar, to not have to stop generation. 

\subsection{Risks and Consequences}
The risks that come with lowering the safety threshold is to affect the generation of the LLM, and it may detect small things as potentially dangerous and avoid generating content altogether. The LLM should be instead trained or prompted in a way to detect such features in the context or user query, and act in an appropriate manner to mitigate or resolve the safety risk. It can advise the user to take a step towards a correct action, instead of the queried, potentially "dangerous" action or thought. Most of the LLMs are usually built with safeguarding on their server-side, however there have been instances where users were able to manipulate the LLMs to provide inappropriate guidances or solutions. In summary, instead of aborting generation due to a detected safety concern, it should still generate, and advise the user to think in the correct direction, and/or avoid answering on the topic, with the reason of it being an inappropriate request, and asking the user to ask something else.

\section{JSON Output Assessment}
Does the chatbot provide accurate descriptions given the instructions? Is the JSON format correct? Fields correctly filled? Answers relevant? Summary corresponds to JSON response? 

\begin{center}
\includegraphics[scale=0.25]{images/q7-8-json-response.png}    
\end{center}

The returned JSON output seems correct, however the entry characters \texttt{```json}, and \texttt{```} were returned by the LLM in every response. The syntax seemed to be correct in all of the cases, with other mushroom images as well. It did, however, follow those instructions in some iterations, and just for it to be presentable and human-readable on Gradio, it was left to be with the markdown brackets. As shown in the code attached at the end of the report, an if-block was created that checks whether a file has been provided, AND the content from the Textblock was empty (`'). If that is true, then a certain \texttt{system\_instructions} prompt is given, for facilitating a JSON response as an answer, otherwise the default introductory prompt is given, saying \texttt{`You are an assistant bot that is only to discuss about mushrooms. You ...'}. As per the instructions, it was instructed to give the object with only a fixed set of attributes, and was also prompted on what it should fill in for each field. 

\begin{center}
\includegraphics[scale=0.23]{images/json-prompttemplate.png}    
\end{center}

\section{Answer Quality Check}
Can you check the quality of the chatbot's answers? How? Try \verb|mushroom_*.jpg| images with various models of various complexities. What are the results? What are the limitations of a mushroom expert chatbot?

\textbf{Memory:} If there was just an image sent, for example, then the model has recorded its generated JSON information object in its history, and can still answer following questions by the user. To test this, I had first uploaded the image, and sent an empty text. It makes note of it, and prints the JSON on both the UI as well as on the Python kernel terminal on which it is running. I then removed the image, and then sent just a text question, testing its history, by asking "What was the genus of the mushroom?", and it provided correctly.

\includegraphics[width=\textwidth]{images/testing-memory.png}

However, there were some moments where the LLM generated different information altogether, about the subject that was being asked, but not answering the main question. 

\includegraphics[width=\textwidth]{images/wrong-answering.png}

Next, for trying models with different complexities, I tried \texttt{gemini-2.5-flash}, \texttt{gemini-2.5-pro}, and \texttt{gemma-3}. \texttt{gemini-2.5-flash} was slightly slower in generation, taking around 2-3 seconds more in JSON generation than \texttt{gemini-2.0-flash}. It still mis-classified the false-chanterelle (mushroom 3)) to the "Jack O'lantern Mushroom". After trying \texttt{gemini-2.5-pro}, 3) was now classified as "Spectacular Rustgill", which was again incorrect, and was slower than both of the previous 2.0 and 2.5 flash models. 

Finally, I tried the Gemma 3 model, specifically \texttt{gemma-3-27b-it} on the same mushroom. A tradeoff, however, was that a user-defined configuration could not be provided, which means the \texttt{system\_instruction} given at the start, would have to now be provided with every request to it, which can take a lot of token count. It now provided the correct classification, which was great (shown in the image below). The temperature was set to be the default value on the \texttt{gr.Slider}, being 0.7. 

\includegraphics[width=\textwidth]{images/gemma3-mushroom3.png}

The amount of weights that are present in the model architecture, and the fact that some Gemini models have Google-search enhanced responses plays a role in the answer quality. Additionally, the speed of answer generation can tradeoff the quality of the answer as well, where the Pro, or the Gemma 3 can give a slightly more accurate answer, albeit slower than the flash models. 

\section{Dangerous Knowledge Handling}

Luckily, mushroom 6) is a very close image to the Amanita Muscaria mushroom. Upon prompting that to the default model of the assignment \texttt{gemini-2.0-flash} model, it gave edibility instructions accordingly. It initially claims that the mushroom is not to be eaten. 

If the user wants to know from the chatbot if a given mushroom can be edible in some other form, when it is inedible originally, it needs to be asked in a way that makes the LLM check that aspect. I had asked 'Can I consume this? What should I keep in mind before eating it?', but can be extended with 'Can it become edible after cooking?', or 'Is it edible in another form?'. 

\includegraphics[width=\textwidth]{images/q9-dangerous.png}

Doing this made it respond correctly, however was still wary about recommending the user to eat it.

\includegraphics[width=\textwidth]{images/q9-pt2.png}

For additional precautions for the model to make sure to not provide this to inexperienced people, so that they eat it, it can retrieve knowledge in some form, potentially by books or pages about the toxic types of mushrooms, where it is written that there are still ways of eating them safely, and from which the LLM can refer to. If the book excerpt or page given to the LLM mentions specific cooking methods, then the LLM will also be able to provide it out to the user as a response. 

\subsection*{Usage of Generative AI}
To answer the questions part of this hand-in assignment, further Generative AI assistant tools were not used to help the student devise solutions to the implementation and process. Long live human intelligence as well!

\subsection*{References}
\small \url{https://ai.google.dev/gemini-api/docs/safety-settings}

\url{https://www.gradio.app/guides/creating-a-chatbot-fast}

\url{https://www.gradio.app/docs/gradio/chatinterface}

\footnotesize \url{https://www.gradio.app/guides/agents-and-tool-usage#a-real-example-using-gemini-2-0-flash-thinking-api}

\appendix
\newpage
\section{Code}
\label{app:code}
\lstinputlisting[inputencoding=latin1,language=Python,columns=fullflexible]{mushroom_chatbot.py}    

%\inputminted[frame=lines,framesep=2mm,linenos]{python}{mushroom_chatbot.py}

\newpage
\section{Chat Examples}
Insert transcripts or screenshots of chatbot responses.

\subsection{Mushroom 1}
\label{img:1}
\includegraphics[width=\textwidth]{images/mushroom1-2.png}
\includegraphics[width=\textwidth]{images/mushroom1-3.png}
\includegraphics[width=\textwidth]{images/mushroom1-1.png}

\newpage
\subsection{Mushroom 2}
\label{img:2}
\includegraphics[width=0.51\textwidth]{images/mushroom2-2.png}
\includegraphics[width=0.49\textwidth]{images/mushroom2-3.png}
\includegraphics[width=\textwidth]{images/mushroom2-misclassified.png}
\includegraphics[width=\textwidth]{images/mushroom2-1.png}

\newpage
\subsection{Mushroom 3}
\label{img:3}
\includegraphics[width=\textwidth]{images/mushroom3-2.png}
\includegraphics[width=\textwidth]{images/mushroom3-3.png}
\includegraphics[width=\textwidth]{images/mushroom3-1.png}

\newpage
\subsection{Mushroom 4}
\label{img:4}
\includegraphics[width=\textwidth]{images/mushroom4-2.png}
\includegraphics[width=\textwidth]{images/mushroom4-4.png}
\includegraphics[width=\textwidth]{images/mushroom4-1.png}

\subsection{Mushroom 5}
\label{img:5}
\begin{center}
    \includegraphics[width=0.8\textwidth]{images/mushroom5-2.png}
    \includegraphics[width=0.8\textwidth]{images/mushroom5-3.png}
\end{center}

\subsection{Mushroom 6}
\label{img:6}
\begin{center}
    \includegraphics[width=0.8\textwidth]{images/mushroom6-1.png}
    \includegraphics[width=0.8\textwidth]{images/mushroom6-2.png}
\end{center}

\end{document}
